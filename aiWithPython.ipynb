{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#exp1- Depth First Search in python programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "4\n",
      "2\n",
      "3\n",
      "3\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0', '1', '2', '3', '4'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DFS algorithm in Python\n",
    "\n",
    "\n",
    "# DFS algorithm\n",
    "def dfs(graph, start, visited=None):\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "    visited.add(start)\n",
    "\n",
    "    print(start)\n",
    "\n",
    "    for next in graph[start] - visited:\n",
    "        dfs(graph, next, visited)\n",
    "    return visited\n",
    "\n",
    "\n",
    "graph = {'0': set(['1', '2']),\n",
    "         '1': set(['0', '3', '4']),\n",
    "         '2': set(['0']),\n",
    "         '3': set(['1']),\n",
    "         '4': set(['2', '3'])}\n",
    "\n",
    "dfs(graph, '0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#exp2- Breadth First Serach in python programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following is Breadth First Traversal: \n",
      "0 1 2 3 "
     ]
    }
   ],
   "source": [
    "# BFS algorithm in Python\n",
    "\n",
    "\n",
    "import collections\n",
    "\n",
    "# BFS algorithm\n",
    "def bfs(graph, root):\n",
    "\n",
    "    visited, queue = set(), collections.deque([root])\n",
    "    visited.add(root)\n",
    "\n",
    "    while queue:\n",
    "\n",
    "        # Dequeue a vertex from queue\n",
    "        vertex = queue.popleft()\n",
    "        print(str(vertex) + \" \", end=\"\")\n",
    "\n",
    "        # If not visited, mark it as visited, and\n",
    "        # enqueue it\n",
    "        for neighbour in graph[vertex]:\n",
    "            if neighbour not in visited:\n",
    "                visited.add(neighbour)\n",
    "                queue.append(neighbour)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    graph = {0: [1, 2], 1: [2], 2: [3], 3: [1, 2]}\n",
    "    print(\"Following is Breadth First Traversal: \")\n",
    "    bfs(graph, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#exp3- Implemenatation of Perceptron AND Logic Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: (0, 0)  Output: 0\n",
      "Input: (0, 1)  Output: 0\n",
      "Input: (1, 0)  Output: 0\n",
      "Input: (1, 1)  Output: 1\n"
     ]
    }
   ],
   "source": [
    "w1, w2, b = 0.5, 0.5, -1\n",
    "\n",
    "def activate(x):\n",
    "    return 1 if x >= 0 else 0\n",
    "\n",
    "def train_perceptron(inputs, desired_outputs, learning_rate, epochs):\n",
    "    global w1, w2, b  \n",
    "    for epoch in range(epochs):\n",
    "        total_error = 0\n",
    "        for i in range(len(inputs)):\n",
    "            A, B = inputs[i]\n",
    "            target_output = desired_outputs[i]\n",
    "            output = activate(w1 * A + w2 * B + b)\n",
    "            error = target_output - output\n",
    "            w1 += learning_rate * error * A\n",
    "            w2 += learning_rate * error * B\n",
    "            b += learning_rate * error\n",
    "            total_error += abs(error)\n",
    "        if total_error == 0:\n",
    "            break\n",
    "\n",
    "\n",
    "inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "desired_outputs = [0, 0, 0, 1]\n",
    "learning_rate = 0.1\n",
    "epochs = 100\n",
    "\n",
    "train_perceptron(inputs, desired_outputs, learning_rate, epochs)\n",
    "\n",
    "for i in range(len(inputs)):\n",
    "    A, B = inputs[i]\n",
    "    output = activate(w1 * A + w2 * B + b)\n",
    "    print(f\"Input: ({A}, {B})  Output: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exp4- Implemenatation of Perceptron OR Logic Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Perceptron OR gate:\n",
      "Input: [0 0], Output: 0\n",
      "Input: [0 1], Output: 1\n",
      "Input: [1 0], Output: 1\n",
      "Input: [1 1], Output: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def step_function(x):\n",
    "    return 1 if x >= 0 else 0\n",
    "\n",
    "\n",
    "class PerceptronOR:\n",
    "    def __init__(self, input_size):\n",
    "        self.weights = np.random.rand(input_size)\n",
    "        self.bias = np.random.rand()\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        summation = np.dot(inputs, self.weights) + self.bias\n",
    "        return step_function(summation)\n",
    "\n",
    "    def train(self, inputs, target_output, learning_rate=0.1, epochs=100):\n",
    "        for epoch in range(epochs):\n",
    "            total_error = 0\n",
    "            for input_data, target in zip(inputs, target_output):\n",
    "                prediction = self.predict(input_data)\n",
    "                error = target - prediction\n",
    "                total_error += abs(error)\n",
    "                self.weights += learning_rate * error * input_data\n",
    "                self.bias += learning_rate * error\n",
    "            if total_error == 0:\n",
    "                break\n",
    "\n",
    "\n",
    "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "target_output = np.array([0, 1, 1, 1])\n",
    "\n",
    "\n",
    "or_gate = PerceptronOR(input_size=2)\n",
    "or_gate.train(inputs, target_output)\n",
    "\n",
    "\n",
    "print(\"Testing Perceptron OR gate:\")\n",
    "for input_data in inputs:\n",
    "    output = or_gate.predict(input_data)\n",
    "    print(f\"Input: {input_data}, Output: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#exp5- Implemenatation of Perceptron AND Logic Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAND(0, 1) = 1\n",
      "NAND(1, 1) = 0\n",
      "NAND(0, 0) = 1\n",
      "NAND(1, 0) = 1\n"
     ]
    }
   ],
   "source": [
    "# importing Python library \n",
    "import numpy as np \n",
    "\n",
    "# define Unit Step Function \n",
    "def unitStep(v): \n",
    "\tif v >= 0: \n",
    "\t\treturn 1\n",
    "\telse: \n",
    "\t\treturn 0\n",
    "\n",
    "# design Perceptron Model \n",
    "def perceptronModel(x, w, b): \n",
    "\tv = np.dot(w, x) + b \n",
    "\ty = unitStep(v) \n",
    "\treturn y \n",
    "\n",
    "# NOT Logic Function \n",
    "# wNOT = -1, bNOT = 0.5 \n",
    "def NOT_logicFunction(x): \n",
    "\twNOT = -1\n",
    "\tbNOT = 0.5\n",
    "\treturn perceptronModel(x, wNOT, bNOT) \n",
    "\n",
    "# AND Logic Function \n",
    "# w1 = 1, w2 = 1, bAND = -1.5 \n",
    "def AND_logicFunction(x): \n",
    "\tw = np.array([1, 1]) \n",
    "\tbAND = -1.5\n",
    "\treturn perceptronModel(x, w, bAND) \n",
    "\n",
    "# NAND Logic Function \n",
    "# with AND and NOT \n",
    "# function calls in sequence \n",
    "def NAND_logicFunction(x): \n",
    "\toutput_AND = AND_logicFunction(x) \n",
    "\toutput_NOT = NOT_logicFunction(output_AND) \n",
    "\treturn output_NOT \n",
    "\n",
    "# testing the Perceptron Model \n",
    "test1 = np.array([0, 1]) \n",
    "test2 = np.array([1, 1]) \n",
    "test3 = np.array([0, 0]) \n",
    "test4 = np.array([1, 0]) \n",
    "\n",
    "print(\"NAND({}, {}) = {}\".format(0, 1, NAND_logicFunction(test1))) \n",
    "print(\"NAND({}, {}) = {}\".format(1, 1, NAND_logicFunction(test2))) \n",
    "print(\"NAND({}, {}) = {}\".format(0, 0, NAND_logicFunction(test3))) \n",
    "print(\"NAND({}, {}) = {}\".format(1, 0, NAND_logicFunction(test4))) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#exp6- Implemenatation of Perceptron NOR Logic Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOR(0, 1) = 0\n",
      "NOR(1, 1) = 0\n",
      "NOR(0, 0) = 1\n",
      "NOR(1, 0) = 0\n"
     ]
    }
   ],
   "source": [
    "# importing Python library \n",
    "import numpy as np \n",
    "\n",
    "# define Unit Step Function \n",
    "def unitStep(v): \n",
    "\tif v >= 0: \n",
    "\t\treturn 1\n",
    "\telse: \n",
    "\t\treturn 0\n",
    "\n",
    "# design Perceptron Model \n",
    "def perceptronModel(x, w, b): \n",
    "\tv = np.dot(w, x) + b \n",
    "\ty = unitStep(v) \n",
    "\treturn y \n",
    "\n",
    "# NOT Logic Function \n",
    "# wNOT = -1, bNOT = 0.5 \n",
    "def NOT_logicFunction(x): \n",
    "\twNOT = -1\n",
    "\tbNOT = 0.5\n",
    "\treturn perceptronModel(x, wNOT, bNOT) \n",
    "\n",
    "# OR Logic Function \n",
    "# w1 = 1, w2 = 1, bOR = -0.5 \n",
    "def OR_logicFunction(x): \n",
    "\tw = np.array([1, 1]) \n",
    "\tbOR = -0.5\n",
    "\treturn perceptronModel(x, w, bOR) \n",
    "\n",
    "# NOR Logic Function \n",
    "# with OR and NOT \n",
    "# function calls in sequence \n",
    "def NOR_logicFunction(x): \n",
    "\toutput_OR = OR_logicFunction(x) \n",
    "\toutput_NOT = NOT_logicFunction(output_OR) \n",
    "\treturn output_NOT \n",
    "\n",
    "# testing the Perceptron Model \n",
    "test1 = np.array([0, 1]) \n",
    "test2 = np.array([1, 1]) \n",
    "test3 = np.array([0, 0]) \n",
    "test4 = np.array([1, 0]) \n",
    "\n",
    "print(\"NOR({}, {}) = {}\".format(0, 1, NOR_logicFunction(test1))) \n",
    "print(\"NOR({}, {}) = {}\".format(1, 1, NOR_logicFunction(test2))) \n",
    "print(\"NOR({}, {}) = {}\".format(0, 0, NOR_logicFunction(test3))) \n",
    "print(\"NOR({}, {}) = {}\".format(1, 0, NOR_logicFunction(test4))) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#exp7- Implemenatation of Perceptron XOR Logic Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR(0, 0) = 1.0\n",
      "XOR(0, 1) = 0.0\n",
      "XOR(1, 0) = 1.0\n",
      "XOR(1, 1) = 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Function for XOR operation\n",
    "def XOR_Logic(x):\n",
    "    # Input to hidden layer weights and bias\n",
    "    w_input_hidden = np.array([[20, 20], [-20, -20]])\n",
    "    b_input_hidden = np.array([-30, 10])\n",
    "    \n",
    "    # Hidden to output layer weights and bias\n",
    "    w_hidden_output = np.array([20, 20])\n",
    "    b_hidden_output = -10\n",
    "    \n",
    "    # Calculate output of hidden layer\n",
    "    hidden_layer_output = sigmoid(np.dot(x, w_input_hidden) + b_input_hidden)\n",
    "    \n",
    "    # Calculate output of output layer\n",
    "    output = sigmoid(np.dot(hidden_layer_output, w_hidden_output) + b_hidden_output)\n",
    "    \n",
    "    return np.round(output)  # Round to 0 or 1\n",
    "\n",
    "# Test cases\n",
    "test_cases = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "\n",
    "# Testing\n",
    "for test in test_cases:\n",
    "    result = XOR_Logic(test)\n",
    "    print(\"XOR({}, {}) = {}\".format(test[0], test[1], result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#exp8- Implemenatation of Perceptron XNOR Logic Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XNOR(0, 0) = 0.0\n",
      "XNOR(0, 1) = 0.0\n",
      "XNOR(1, 0) = 0.0\n",
      "XNOR(1, 1) = 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Function for XNOR operation\n",
    "def XNOR_Logic(x):\n",
    "    # Input to hidden layer weights and bias\n",
    "    w_input_hidden = np.array([[20, 20], [-20, -20]])\n",
    "    b_input_hidden = np.array([-30, 10])\n",
    "    \n",
    "    # Hidden to output layer weights and bias\n",
    "    w_hidden_output = np.array([20, 20])\n",
    "    b_hidden_output = -30\n",
    "    \n",
    "    # Calculate output of hidden layer\n",
    "    hidden_layer_output = sigmoid(np.dot(x, w_input_hidden) + b_input_hidden)\n",
    "    \n",
    "    # Calculate output of output layer\n",
    "    output = sigmoid(np.dot(hidden_layer_output, w_hidden_output) + b_hidden_output)\n",
    "    \n",
    "    return np.round(output)  # Round to 0 or 1\n",
    "\n",
    "# Test cases\n",
    "test_cases = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "\n",
    "# Testing\n",
    "for test in test_cases:\n",
    "    result = XNOR_Logic(test)\n",
    "    print(\"XNOR({}, {}) = {}\".format(test[0], test[1], result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
